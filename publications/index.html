<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Jiashu Xu 徐家澍 </title> <meta name="author" content="Jiashu Xu 徐家澍"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar. see latest work on &lt;a href='https://scholar.google.com/citations?user=0uYehJsAAAAJ'&gt;&lt;u&gt;google scholar&lt;/u&gt;&lt;/a&gt; page."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, jiashu, jiashu-xu"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8C%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cnut1648.github.io//publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jiashu</span> Xu 徐家澍 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/files/Jiashu_Xu_CV.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar. see latest work on <a href="https://scholar.google.com/citations?user=0uYehJsAAAAJ" rel="external nofollow noopener" target="_blank"><u>google scholar</u></a> page.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nvidia-cosmos-header-480.webp 480w,/assets/img/publication_preview/nvidia-cosmos-header-800.webp 800w,/assets/img/publication_preview/nvidia-cosmos-header-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/nvidia-cosmos-header.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="nvidia-cosmos-header.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="agarwal2025cosmos" class="col-sm-8"> <div class="title">Cosmos World Foundation Model Platform for Physical AI</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 <strong style="color: red;">(CES’25 Best of AI, Best Overall)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nvidia.com/en-us/ai/cosmos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2501.03575" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=9Uch931cDx8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/NVIDIA/Cosmos" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via NVIDIA Cosmos-Predict1.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">agarwal2025cosmos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cosmos World Foundation Model Platform for Physical AI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://www.nvidia.com/en-us/ai/cosmos/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cosmos-predict2.5-480.webp 480w,/assets/img/publication_preview/cosmos-predict2.5-800.webp 800w,/assets/img/publication_preview/cosmos-predict2.5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/cosmos-predict2.5.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="cosmos-predict2.5.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="ali2025world" class="col-sm-8"> <div class="title">World simulation with video foundation models for physical ai</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://research.nvidia.com/labs/dir/cosmos-predict2.5/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2511.00062" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI visionlanguage model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5× smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ali2025world</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{World simulation with video foundation models for physical ai}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://research.nvidia.com/labs/dir/cosmos-predict2.5/}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cosmos-reason1-480.webp 480w,/assets/img/publication_preview/cosmos-reason1-800.webp 800w,/assets/img/publication_preview/cosmos-reason1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/cosmos-reason1.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="cosmos-reason1.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="azzolini2025cosmos" class="col-sm-8"> <div class="title">Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nvidia.com/en-us/ai/cosmos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2503.15558" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtube.com/live/TLzna9__DnI?feature=shared&amp;t=1993" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/nvidia-cosmos/cosmos-reason1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, CosmosReason1-7B and Cosmos-Reason1-56B. We curate data and train our models in two stages: Physical AI supervised fine-tuning (SFT) and Physical AI reinforcement learning (RL). To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and RL bring significant improvements. To facilitate the development of Physical AI, we make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">azzolini2025cosmos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://www.nvidia.com/en-us/ai/cosmos/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ddrl-480.webp 480w,/assets/img/publication_preview/ddrl-800.webp 800w,/assets/img/publication_preview/ddrl-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/ddrl.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="ddrl.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr> </div> </div> <div id="ye2025data" class="col-sm-8"> <div class="title">Data-regularized Reinforcement Learning for Diffusion Models at Scale</div> <div class="author"> <a href="https://haotianye.com/" rel="external nofollow noopener" target="_blank">Haotian Ye</a>, <a href="https://zhengkw18.github.io/" rel="external nofollow noopener" target="_blank">Kaiwen Zheng</a>, <em>Jiashu Xu</em> , <a href="https://leo-li.com/" rel="external nofollow noopener" target="_blank">Puheng Li</a> , <a href="https://chendrag.github.io/" rel="external nofollow noopener" target="_blank">Huayu Chen</a>, <a href="https://hanjq17.github.io/" rel="external nofollow noopener" target="_blank">Jiaqi Han</a> , <a href="https://shengliu66.github.io/" rel="external nofollow noopener" target="_blank">Sheng Liu</a> , <a href="https://qsh-zh.github.io/" rel="external nofollow noopener" target="_blank">Qinsheng Zhang</a>, <a href="https://hanzimao.me/" rel="external nofollow noopener" target="_blank">Hanzi Mao</a>, <a href="https://zekunhao.com/" rel="external nofollow noopener" target="_blank">Zekun Hao</a>, <a href="https://prithv1.xyz/" rel="external nofollow noopener" target="_blank">Prithvijit Chattopadhyay</a> , <a href="https://dinghow.site/" rel="external nofollow noopener" target="_blank">Dinghao Yang</a>, <a href="https://research.nvidia.com/labs/avg/author/liang-feng/" rel="external nofollow noopener" target="_blank">Liang Feng</a>, <a href="https://github.com/foreverlms" rel="external nofollow noopener" target="_blank">Maosheng Liao</a>, <a href="https://scholar.google.com/citations?user=gSB8_64AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Junjie Bai</a> , <a href="https://mingyuliu.net/" rel="external nofollow noopener" target="_blank">Ming-Yu Liu</a>, <a href="https://www.james-zou.com/" rel="external nofollow noopener" target="_blank">James Zou</a>, and <a href="https://cs.stanford.edu/~ermon/" rel="external nofollow noopener" target="_blank">Stefano Ermon</a> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://research.nvidia.com/labs/dir/ddrl/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2512.04332" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ye2025data</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data-regularized Reinforcement Learning for Diffusion Models at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ye, Haotian and Zheng, Kaiwen and Xu, Jiashu and Li, Puheng and Chen, Huayu and Han, Jiaqi and Liu, Sheng and Zhang, Qinsheng and Mao, Hanzi and Hao, Zekun and Chattopadhyay, Prithvijit and Yang, Dinghao and Feng, Liang and Liao, Maosheng and Bai, Junjie and Liu, Ming-Yu and Zou, James and Ermon, Stefano}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://research.nvidia.com/labs/dir/ddrl/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cosmos-transfer-480.webp 480w,/assets/img/publication_preview/cosmos-transfer-800.webp 800w,/assets/img/publication_preview/cosmos-transfer-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/cosmos-transfer.gif" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="cosmos-transfer.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="alhaija2025cosmos" class="col-sm-8"> <div class="title">Cosmos-transfer1: Conditional world generation with adaptive multimodal control</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nvidia.com/en-us/ai/cosmos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2503.14492" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nvidia-cosmos/cosmos-transfer1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">alhaija2025cosmos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cosmos-transfer1: Conditional world generation with adaptive multimodal control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://www.nvidia.com/en-us/ai/cosmos/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dreamdistribution-480.webp 480w,/assets/img/publication_preview/dreamdistribution-800.webp 800w,/assets/img/publication_preview/dreamdistribution-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/dreamdistribution.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="dreamdistribution.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a></abbr> </div> </div> <div id="zhao2023dream" class="col-sm-8"> <div class="title">DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models</div> <div class="author"> <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a> , <a href="https://www.linkedin.com/in/mydcxiao/?locale=en_US" rel="external nofollow noopener" target="_blank">Yuhang Xiao*</a>, <em>Jiashu Xu*</em>, <a href="https://www.microsoft.com/en-us/research/people/xinyangjiang/" rel="external nofollow noopener" target="_blank">Xinyang Jiang</a>, <a href="https://www.microsoft.com/en-us/research/people/yifanyang/" rel="external nofollow noopener" target="_blank">Yifan Yang</a> , <a href="http://recmind.cn/" rel="external nofollow noopener" target="_blank">Dongsheng Li</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations (ICLR)</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://briannlongzhao.github.io/DreamDistribution/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="http://arxiv.org/abs/2312.14216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/briannlongzhao/DreamDistribution" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The popularization of Text-to-Image (T2I) diffusion models enables the generation of high-quality images from text descriptions. However, generating diverse customized images with reference visual attributes remains challenging. This work focuses on personalizing T2I diffusion models at a more abstract concept or category level, adapting commonalities from a set of reference images while creating new instances with sufficient variations. We introduce a solution that allows a pretrained T2I diffusion model to learn a set of soft prompts, enabling the generation of novel images by sampling prompts from the learned distribution. These prompts offer text-guided editing capabilities and additional flexibility in controlling variation and mixing between multiple distributions. We also show the adaptability of the learned prompt distribution to other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our approach through quantitative analysis including automatic evaluation and human assessment. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhao2023dream</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Brian Nlong and Xiao, Yuhang and Xu, Jiashu and Jiang, Xinyang and Yang, Yifan and Li, Dongsheng and Itti, Laurent and Ge, Yunhao and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://briannlongzhao.github.io/DreamDistribution/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/demo_defense-480.webp 480w,/assets/img/publication_preview/demo_defense-800.webp 800w,/assets/img/publication_preview/demo_defense-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/demo_defense.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="demo_defense.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> </div> </div> <div id="mo2023testtime" class="col-sm-8"> <div class="title">Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations</div> <div class="author"> <a href="https://jacky-mo-1111.github.io/" rel="external nofollow noopener" target="_blank">Wenjie Mo</a>, <em>Jiashu Xu</em>, <a href="https://qinliu9.github.io/" rel="external nofollow noopener" target="_blank">Qin Liu</a> , <a href="https://jiongxiao-wang.github.io/" rel="external nofollow noopener" target="_blank">Jiongxiao Wang</a>, <a href="https://junyann.github.io/" rel="external nofollow noopener" target="_blank">Jun Yan</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2311.09763" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes particularly pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, our work introduces defensive demonstrations, an innovative backdoor defense strategy for blackbox large language models. Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications/tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations. Extensive experiments show that defensive demonstrations are effective in defending both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mo2023testtime</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mo, Wenjie and Xu, Jiashu and Liu, Qin and Wang, Jiongxiao and Yan, Jun and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fingerprintv2-480.webp 480w,/assets/img/publication_preview/fingerprintv2-800.webp 800w,/assets/img/publication_preview/fingerprintv2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/fingerprintv2.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="fingerprintv2.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu2024instructional" class="col-sm-8"> <div class="title">Instructional Fingerprinting of Large Language Models</div> <div class="author"> <em>Jiashu Xu</em>, <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang*</a> , <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma*</a>, <a href="https://koh.pw/" rel="external nofollow noopener" target="_blank">Pang Wei Koh</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , 2024 <strong style="color: red;">(Oral)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cnut1648.github.io/Model-Fingerprint/" class="btn btn-sm z-depth-0" role="button">Project Page</a> <a href="https://aclanthology.org/2024.naacl-long.180/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/cnut1648/Model-Fingerprint" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (\eg restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2024instructional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Instructional Fingerprinting of Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Wang, Fei and Ma, Mingyu Derek and Koh, Pang Wei and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Oral}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://cnut1648.github.io/Model-Fingerprint/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/BVS-480.webp 480w,/assets/img/publication_preview/BVS-800.webp 800w,/assets/img/publication_preview/BVS-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/BVS.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="BVS.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Highlight</abbr> </div> </div> <div id="ge2024behavior" class="col-sm-8"> <div class="title">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <a href="https://tangyihe.com/" rel="external nofollow noopener" target="_blank">Yihe Tang*</a>, <em>Jiashu Xu*</em>, <a href="https://www.cemgokmen.com/" rel="external nofollow noopener" target="_blank">Cem Gokmen*</a> , <a href="https://www.chengshuli.me/" rel="external nofollow noopener" target="_blank">Chengshu Li</a>, <a href="https://wensi-ai.github.io/" rel="external nofollow noopener" target="_blank">Wensi Ai</a>, <a href="https://web.stanford.edu/~benjm/" rel="external nofollow noopener" target="_blank">Benjamin Jose Martinez</a>, <a href="https://www.linkedin.com/in/arman-aydin-915035185/" rel="external nofollow noopener" target="_blank">Arman Aydin</a>, <a href="https://www.linkedin.com/in/mona-anvari/" rel="external nofollow noopener" target="_blank">Mona Anvari</a>, <a href="https://scholar.google.ca/citations?user=u4S8E4UAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ayush K Chakravarthy</a>, <a href="https://kovenyu.com/" rel="external nofollow noopener" target="_blank">Hong-Xing Yu</a>, <a href="https://jdw.ong/" rel="external nofollow noopener" target="_blank">Josiah Wong</a>, <a href="https://scholar.google.com/citations?user=sqTh_dwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sanjana Srivastava</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=jGwt3mcAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sharon Lee</a>, <a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Shengxin Zha</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a> , <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, <a href="https://robertomartinmartin.com/" rel="external nofollow noopener" target="_blank">Roberto Martin-Martin</a> , <a href="https://aptx4869lm.github.io/" rel="external nofollow noopener" target="_blank">Miao Liu</a>, <a href="https://pzzhang.github.io/pzzhang/" rel="external nofollow noopener" target="_blank">Pengchuan Zhang</a> , <a href="https://ai.stanford.edu/~zharu/" rel="external nofollow noopener" target="_blank">Ruohan Zhang</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2024 <strong style="color: red;">(Highlight)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://behavior-vision-suite.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as "filled" and "folded"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ge2024behavior</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Tang, Yihe and Xu, Jiashu and Gokmen, Cem and Li, Chengshu and Ai, Wensi and Martinez, Benjamin Jose and Aydin, Arman and Anvari, Mona and Chakravarthy, Ayush K and Yu, Hong-Xing and Wong, Josiah and Srivastava, Sanjana and Lee, Sharon and Zha, Shengxin and Itti, Laurent and Li, Yunzhu and Martin-Martin, Roberto and Liu, Miao and Zhang, Pengchuan and Zhang, Ruohan and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Highlight}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://behavior-vision-suite.github.io/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/genusd-480.webp 480w,/assets/img/publication_preview/genusd-800.webp 800w,/assets/img/publication_preview/genusd-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/genusd.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="genusd.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://www.siggraph.org/" rel="external nofollow noopener" target="_blank">SIGGRAPH</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Real-Time Live!</abbr> </div> </div> <div id="lin2024genusd" class="col-sm-8"> <div class="title">Genusd: 3d scene generation made easy</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>In ACM SIGGRAPH</em> , 2024 <strong style="color: red;">(Real-Time Live!)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&amp;linkId=100000277255797" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.youtube.com/watch?v=Gm1B5DT8kE0&amp;t=1972s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We introduce GenUSD, an end-to-end text-to-scene generation framework that transforms natural language queries into realistic 3D scenes, including 3D objects and layouts. The process involves two main steps: 1) A Large Language Model (LLM) generates a scene layout hierarchically. It first proposes a high-level plan to decompose the scene into multiple functionally and spatially distinct subscenes. Then, for each subscene, the LLM proposes objects with detailed positions, poses, sizes, and descriptions. To manage complex object relationships and intricate scenes, we introduce object layout design meta functions as tools for the LLM. 2) A novel text-to-3D model generates each 3D object with surface meshes and high-resolution texture maps based on the LLM’s descriptions. The assembled 3D assets form the final 3D scene, represented as a Universal Scene Description (USD) format. GenUSD ensures physical plausibility by incorporating functions to prevent collisions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">lin2024genusd</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Genusd: 3d scene generation made easy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Real-Time Live!}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--2}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&amp;linkId=100000277255797}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/edify3d-480.webp 480w,/assets/img/publication_preview/edify3d-800.webp 800w,/assets/img/publication_preview/edify3d-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/edify3d.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="edify3d.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="bala2024edify" class="col-sm-8"> <div class="title">Edify 3D: Scalable High-Quality 3D Asset Generation</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://research.nvidia.com/labs/dir/edify-3d/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2411.07135" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=ROqB8xhKZ6U" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We introduce Edify 3D, an advanced solution designed for high-quality 3D asset generation. Our method first synthesizes RGB and surface normal images of the described object at multiple viewpoints using a diffusion model. The multi-view observations are then used to reconstruct the shape, texture, and PBR materials of the object. Our method can generate high-quality 3D assets with detailed geometry, clean shape topologies, high-resolution textures, and materials within 2 minutes of runtime.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bala2024edify</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Edify 3D: Scalable High-Quality 3D Asset Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://research.nvidia.com/labs/dir/edify-3d/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mitigating-backdoor-threats-480.webp 480w,/assets/img/publication_preview/mitigating-backdoor-threats-800.webp 800w,/assets/img/publication_preview/mitigating-backdoor-threats-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/mitigating-backdoor-threats.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="mitigating-backdoor-threats.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://allerton.csl.illinois.edu/" rel="external nofollow noopener" target="_blank">Allerton</a></abbr> </div> </div> <div id="liu2024mitigating" class="col-sm-8"> <div class="title">Mitigating backdoor threats to large language models: Advancement and challenges</div> <div class="author"> <a href="https://qinliu9.github.io/" rel="external nofollow noopener" target="_blank">Qin Liu</a>, <a href="https://jacky-mo-1111.github.io/" rel="external nofollow noopener" target="_blank">Wenjie Mo</a>, <a href="https://scholar.google.com/citations?user=UE4CnZcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Terry Tong</a>, <em>Jiashu Xu</em>, <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Annual Allerton Conference on Communication, Control, and Computing</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2409.19993" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The advancement of Large Language Models (LLMs) has significantly impacted various domains, including Web search, healthcare, and software development. However, as these models scale, they become more vulnerable to cybersecurity risks, particularly backdoor attacks. By exploiting the potent memorization capacity of LLMs, adversaries can easily inject backdoors into LLMs by manipulating a small portion of training data, leading to malicious behaviors in downstream applications whenever the hidden backdoor is activated by the pre-defined triggers. Moreover, emerging learning paradigms like instruction tuning and reinforcement learning from human feedback (RLHF) exacerbate these risks as they rely heavily on crowdsourced data and human feedback, which are not fully controlled. In this paper, we present a comprehensive survey of emerging backdoor threats to LLMs that appear during LLM development or inference, and cover recent advancement in both defense and detection strategies for mitigating backdoor threats to LLMs. We also outline key challenges in addressing these threats, highlighting areas for future research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2024mitigating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mitigating backdoor threats to large language models: Advancement and challenges}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Qin and Mo, Wenjie and Tong, Terry and Xu, Jiashu and Wang, Fei and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Allerton Conference on Communication, Control, and Computing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Securing-Multi-turn-Triggers-480.webp 480w,/assets/img/publication_preview/Securing-Multi-turn-Triggers-800.webp 800w,/assets/img/publication_preview/Securing-Multi-turn-Triggers-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/Securing-Multi-turn-Triggers.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="Securing-Multi-turn-Triggers.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://aclanthology.org/venues/emnlp/" rel="external nofollow noopener" target="_blank">EMNLP</a></abbr> </div> </div> <div id="tong2024securing" class="col-sm-8"> <div class="title">Securing multi-turn conversational language models against distributed backdoor triggers</div> <div class="author"> <a href="https://scholar.google.com/citations?user=UE4CnZcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Terry Tong</a>, <em>Jiashu Xu</em>, <a href="https://qinliu9.github.io/" rel="external nofollow noopener" target="_blank">Qin Liu</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2407.04151" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/TerryTong-Git/poisonshare" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have acquired the ability to handle longer context lengths and understand nuances in text, expanding their dialogue capabilities beyond a single utterance. A popular user-facing application of LLMs is the multi-turn chat setting. Though longer chat memory and better understanding may seemingly benefit users, our paper exposes a vulnerability that leverages the multi-turn feature and strong learning ability of LLMs to harm the end-user: the backdoor. We demonstrate that LLMs can capture the combinational backdoor representation. Only upon presentation of triggers together does the backdoor activate. We also verify empirically that this representation is invariant to the position of the trigger utterance. Subsequently, inserting a single extra token into any two utterances of 5% of the data can cause over 99% Attack Success Rate (ASR). Our results with 3 triggers demonstrate that this framework is generalizable, compatible with any trigger in an adversary’s toolbox in a plug-and-play manner. Defending the backdoor can be challenging in the conversational setting because of the large input and output space. Our analysis indicates that the distributed backdoor exacerbates the current challenges by polynomially increasing the dimension of the attacked input space. Canonical textual defenses like ONION and BKI leverage auxiliary model forward passes over individual tokens, scaling exponentially with the input sequence length and struggling to maintain computational feasibility. To this end, we propose a decoding time defense – decayed contrastive decoding – that scales linearly with the assistant response sequence length and reduces the backdoor to as low as 0.35%</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tong2024securing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Securing multi-turn conversational language models against distributed backdoor triggers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tong, Terry and Xu, Jiashu and Liu, Qin and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instruction-attack-480.webp 480w,/assets/img/publication_preview/instruction-attack-800.webp 800w,/assets/img/publication_preview/instruction-attack-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/instruction-attack.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="instruction-attack.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> </div> </div> <div id="xu2023instructions" class="col-sm-8"> <div class="title">Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</div> <div class="author"> <em>Jiashu Xu</em> , <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>, <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cnut1648.github.io/instruction-attack/" class="btn btn-sm z-depth-0" role="button">Project Page</a> <a href="https://aclanthology.org/2024.naacl-long.171/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2023instructions</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Ma, Mingyu Derek and Wang, Fei and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://cnut1648.github.io/instruction-attack/}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/can-nli-480.webp 480w,/assets/img/publication_preview/can-nli-800.webp 800w,/assets/img/publication_preview/can-nli-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/can-nli.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="can-nli.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu-etal-2023-nli" class="col-sm-8"> <div class="title">Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?</div> <div class="author"> <em>Jiashu Xu</em> , <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Association for Computational Linguistics (ACL)</em> , Jul 2023 <strong style="color: red;">(Oral)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2023.acl-long.138.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/luka-group/nli_as_indirect_supervision" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2023-nli</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can {NLI} Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Ma, Mingyu Derek and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Oral}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.138}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.138}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2450--2467}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beyond-generation-480.webp 480w,/assets/img/publication_preview/beyond-generation-800.webp 800w,/assets/img/publication_preview/beyond-generation-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/beyond-generation.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="beyond-generation.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Extension</abbr> </div> </div> <div id="ge2023beyond" class="col-sm-8"> <div class="title">Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <em>Jiashu Xu*</em>, <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a>, <a href="https://neelj.com/" rel="external nofollow noopener" target="_blank">Neel Joshi</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2309.05956</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2309.05956" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/gyhandy/Text2Image-for-Detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data generation into foreground object generation, and contextually coherent background generation. To generate foreground objects, we employ a straightforward textual template, incorporating the object class name as input prompts. This is fed into a text-to-image synthesis framework, producing various foreground images set against isolated backgrounds. A foreground-background segmentation algorithm is then used to generate foreground object masks. To generate context images, we begin by creating language descriptions of the context. This is achieved by applying an image captioning method to a small set of images representing the desired context. These textual descriptions are then transformed into a diverse array of context images via a text-to-image synthesis framework. Subsequently, we composite these with the foreground object masks produced in the initial step, utilizing a cut-and-paste method, to formulate the training data. We demonstrate the advantages of our approach on five object detection and segmentation datasets, including Pascal VOC and COCO. We found that detectors trained solely on synthetic data produced by our method achieve performance comparable to those trained on real data (Fig. 1). Moreover, a combination of real and synthetic data yields even much better results. Further analysis indicates that the synthetic data distribution complements the real data distribution effectively. Additionally, we emphasize the compositional nature of our data generation approach in out-of-distribution and zero-shot data generation scenarios. We open-source our code at https://github.com/gyhandy/Text2Image-for-Detection</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ge2023beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Xu, Jiashu and Zhao, Brian Nlong and Joshi, Neel and Itti, Laurent and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2309.05956}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Extension}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beyond-generation-480.webp 480w,/assets/img/publication_preview/beyond-generation-800.webp 800w,/assets/img/publication_preview/beyond-generation-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/beyond-generation.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="beyond-generation.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr> </div> </div> <div id="ge2022dall" class="col-sm-8"> <div class="title">Dall-e for detection: Language-driven context image synthesis for object detection</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <em>Jiashu Xu*</em>, <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>arXiv preprint</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2206.09592" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/gyhandy/Text2Image-for-Detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-toimage synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach decouples training data generation into foreground object mask generation and background (context) image generation. For foreground object mask generation, we use a simple textual template with object class name as input to DALL-E to generate a diverse set of foreground images. A foreground-background segmentation algorithm is then used to generate foreground object masks. Next, in order to generate context images, first a language description of the context is generated by applying an image captioning method on a small set of images representing the context. These language descriptions are then used to generate diverse sets of context images using the DALL-E framework. These are then composited with object masks generated in the first step to provide an augmented training set for a classifier. We demonstrate the advantages of our approach on four object detection datasets including on Pascal VOC and COCO object detection tasks. Furthermore, we also highlight the compositional nature of our data generation approach on out-of-distribution and zero-shot data generation scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ge2022dall</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dall-e for detection: Language-driven context image synthesis for object detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Xu, Jiashu and Zhao, Brian Nlong and Itti, Laurent and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/X-Norm-480.webp 480w,/assets/img/publication_preview/X-Norm-800.webp 800w,/assets/img/publication_preview/X-Norm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/X-Norm.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="X-Norm.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://icmi.acm.org/" rel="external nofollow noopener" target="_blank">ICMI</a></abbr> </div> </div> <div id="yin2022x" class="col-sm-8"> <div class="title">X-Norm: Exchanging Normalization Parameters for Bimodal Fusion</div> <div class="author"> <a href="https://yufengyin.github.io/" rel="external nofollow noopener" target="_blank">Yufeng Yin*</a>, <em>Jiashu Xu*</em>, <a href="https://www.linkedin.com/in/tianxin-zu-81b13a220" rel="external nofollow noopener" target="_blank">Tianxin Zu</a>, and <a href="https://people.ict.usc.edu/~soleymani/" rel="external nofollow noopener" target="_blank">Mohammad Soleymani</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 International Conference on Multimodal Interaction (ICMI)</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3536221.3556581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Multimodal learning aims to process and relate information from different modalities to enhance the model’s capacity for perception. Current multimodal fusion mechanisms either do not align the feature spaces closely or are expensive for training and inference. In this paper, we present X-Norm, a novel, simple and efficient method for bimodal fusion that generates and exchanges limited but meaningful normalization parameters between the modalities implicitly aligning the feature spaces. We conduct extensive experiments on two tasks of emotion and action recognition with different architectures including Transformer-based and CNN-based models using IEMOCAP and MSP-IMPROV for emotion recognition and EPIC-KITCHENS for action recognition. The experimental results show that X-Norm achieves comparable or superior performance compared to the existing methods including early and late fusion, Gradient-Blending (G-Blend), Tensor Fusion Network, and Multimodal Transformer, with a relatively low training cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yin2022x</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-Norm: Exchanging Normalization Parameters for Bimodal Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yin, Yufeng and Xu, Jiashu and Zu, Tianxin and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 International Conference on Multimodal Interaction (ICMI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{605--614}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/neural-sim-480.webp 480w,/assets/img/publication_preview/neural-sim-800.webp 800w,/assets/img/publication_preview/neural-sim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/neural-sim.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="neural-sim.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://eccv.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a></abbr> </div> </div> <div id="ge2022neural" class="col-sm-8"> <div class="title">Neural-Sim: Learning to Generate Training Data with NeRF</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge</a>, <a href="https://harkiratbehl.github.io/" rel="external nofollow noopener" target="_blank">Harkirat Behl*</a>, <em>Jiashu Xu*</em>, <a href="https://sgunasekar.github.io/" rel="external nofollow noopener" target="_blank">Suriya Gunasekar</a>, <a href="https://neelj.com/" rel="external nofollow noopener" target="_blank">Neel Joshi</a>, <a href="https://people.csail.mit.edu/yalesong/home/" rel="external nofollow noopener" target="_blank">Yale Song</a> , <a href="https://xinw.ai/" rel="external nofollow noopener" target="_blank">Xin Wang</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830463.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/gyhandy/Neural-Sim-NeRF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Training computer vision models usually requires collecting and labeling vast amounts of imagery under a diverse set of scene configurations and properties. This process is incredibly time-consuming, and it is challenging to ensure that the captured data distribution maps well to the target domain of an application scenario. Recently, synthetic data has emerged as a way to address both of these issues. However, existing approaches either require human experts to manually tune each scene property or use automatic methods that provide little to no control; this requires rendering large amounts of random data variations, which is slow and is often suboptimal for the target domain. We present the first fully differentiable synthetic data pipeline that uses Neural Radiance Fields (NeRFs) in a closed-loop with a target application’s loss function. Our approach generates data on-demand, with no human labor, to maximize accuracy for a target task. We illustrate the effectiveness of our method on synthetic and real-world object detection tasks. We also introduce a new "YCB-in-the-Wild" dataset and benchmark that provides a test scenario for object detection with varied poses in real-world environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ge2022neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural-Sim: Learning to Generate Training Data with NeRF}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Behl, Harkirat and Xu, Jiashu and Gunasekar, Suriya and Joshi, Neel and Song, Yale and Wang, Xin and Itti, Laurent and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unist-480.webp 480w,/assets/img/publication_preview/unist-800.webp 800w,/assets/img/publication_preview/unist-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/unist.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="unist.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> </div> </div> <div id="huang-etal-2022-unified" class="col-sm-8"> <div class="title">Unified Semantic Typing with Meaningful Label Inference</div> <div class="author"> <a href="https://jyhuang36.github.io/" rel="external nofollow noopener" target="_blank">James Y. Huang</a>, <a href="https://scholar.google.com/citations?user=UcegV-cAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Bangzheng Li*</a>, <em>Jiashu Xu*</em>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.naacl-main.190.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/luka-group/unist" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Semantic typing aims at classifying tokens or spans of interest in a textual context into semantic categories such as relations, entity types, and event types. The inferred labels of semantic categories meaningfully interpret how machines understand components of text. In this paper, we present UniST, a unified framework for semantic typing that captures label semantics by projecting both inputs and labels into a joint semantic embedding space. To formulate different lexical and relational semantic typing tasks as a unified task, we incorporate task descriptions to be jointly encoded with the input, allowing UniST to be adapted to different tasks without introducing task-specific model components. UniST optimizes a margin ranking loss such that the semantic relatedness of the input and labels is reflected from their embedding similarity. Our experiments demonstrate that UniST achieves strong performance across three semantic typing tasks: entity typing, relation classification and event typing. Meanwhile, UniST effectively transfers semantic knowledge of labels and substantially improves generalizability on inferring rarely seen and unseen types. In addition, multiple semantic typing tasks can be jointly trained within the unified framework, leading to a single compact multi-tasking model that performs comparably to dedicated single-task models, while offering even better transferability.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang-etal-2022-unified</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unified Semantic Typing with Meaningful Label Inference}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, James Y. and Li, Bangzheng and Xu, Jiashu and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 abbr"> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://www.nature.com/npjdigitalmed/" rel="external nofollow noopener" target="_blank">NPJ Digit Med</a></abbr> </div> </div> <div id="ma2021dss" class="col-sm-8"> <div class="title">Dissection Gesture Sequence during Nerve Sparing Predicts Erectile Function Recovery after Robot-Assisted Radical Prostatectomy</div> <div class="author"> <a href="https://scholar.google.com/citations?user=lrFwJtAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Runzhuo Ma</a>, <em>Jiashu Xu</em>, Ivan Rodriguez, Gina DeMeo, Aditya Desai, <a href="https://lqtrinh.com/" rel="external nofollow noopener" target="_blank">Loc Trinh</a>, Jessica Nguyen, <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="external nofollow noopener" target="_blank">Anima Anandkumar</a>, Jim Hu, and <a href="https://researchers.cedars-sinai.edu/Andrew.Hung" rel="external nofollow noopener" target="_blank">Andrew Hung</a> </div> <div class="periodical"> <em>NPJ Digit Medicine</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nature.com/articles/s41746-022-00738-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>How well a surgery is performed impacts a patient’s outcomes; however, objective quantification of performance remains an unsolved challenge. Deconstructing a procedure into discrete instrument-tissue “gestures” is a emerging way to understand surgery. To establish this paradigm in a procedure where performance is the most important factor for patient outcomes, we identify 34,323 individual gestures performed in 80 nerve-sparing robot-assisted radical prostatectomies from two international medical centers. Gestures are classified into nine distinct dissection gestures (e.g., hot cut) and four supporting gestures (e.g., retraction). Our primary outcome is to identify factors impacting a patient’s 1-year erectile function (EF) recovery after radical prostatectomy. We find that less use of hot cut and more use of peel/push are statistically associated with better chance of 1-year EF recovery. Our results also show interactions between surgeon experience and gesture types—similar gesture selection resulted in different EF recovery rates dependent on surgeon experience. To further validate this framework, two teams independently constructe distinct machine learning models using gesture sequences vs. traditional clinical features to predict 1-year EF. In both models, gesture sequences are able to better predict 1-year EF (Team 1: AUC 0.77, 95% CI 0.73–0.81; Team 2: AUC 0.68, 95% CI 0.66–0.70) than traditional clinical features (Team 1: AUC 0.69, 95% CI 0.65–0.73; Team 2: AUC 0.65, 95% CI 0.62–0.68). Our results suggest that gestures provide a granular method to objectively indicate surgical performance and outcomes. Application of this methodology to other surgeries may lead to discoveries on methods to improve surgery.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ma2021dss</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dissection Gesture Sequence during Nerve Sparing Predicts Erectile Function Recovery after Robot-Assisted Radical Prostatectomy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Runzhuo and Xu, Jiashu and Rodriguez, Ivan and DeMeo, Gina and Desai, Aditya and Trinh, Loc and Nguyen, H., Jessica and Anandkumar, Anima and Hu, C., Jim and Hung, J., Andrew}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NPJ Digit Medicine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 abbr"> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://www.auajournals.org/" rel="external nofollow noopener" target="_blank">AUA</a></abbr> </div> </div> <div id="ma2021dart" class="col-sm-8"> <div class="title">Dissection Assessment for Robotic Technique (DART) to Evaluate Nerve-Spare of Robot-Assisted Radical Prostatectomy</div> <div class="author"> <a href="https://scholar.google.com/citations?user=lrFwJtAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Runzhuo Ma</a>, Alvin Hui, <em>Jiashu Xu</em>, Aditya Desai, Michael Tzeng, Emily Cheng, <a href="https://lqtrinh.com/" rel="external nofollow noopener" target="_blank">Loc Trinh</a>, Jessica Nguyen, <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="external nofollow noopener" target="_blank">Anima Anandkumar</a>, Jim Hu, and <a href="https://researchers.cedars-sinai.edu/Andrew.Hung" rel="external nofollow noopener" target="_blank">Andrew Hung</a> </div> <div class="periodical"> <em>American Urological Association Annual Conference (AUA)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.auajournals.org/doi/10.1097/JU.0000000000002607.01" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>High quality nerve-spare (NS) is essential for the preservation of erectile function (EF) after robot-assisted radical prostatectomy (RARP). In a previous study, we developed an assessment tool for tissue dissection, Dissection Assessment for Robotic Technique (DART). Herein, we further apply DART scores to the NS step and evaluate whether DART can predict 1-year EF recovery after RARP.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ma2021dart</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dissection Assessment for Robotic Technique (DART) to Evaluate Nerve-Spare of Robot-Assisted Radical Prostatectomy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Runzhuo and Hui, Alvin and Xu, Jiashu and Desai, Aditya and Tzeng, Michael and Cheng, Emily and Trinh, Loc and Nguyen, H., Jessica and Anandkumar, Anima and Hu, C., Jim and Hung, J., Andrew}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{American Urological Association Annual Conference (AUA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/salkg-480.webp 480w,/assets/img/publication_preview/salkg-800.webp 800w,/assets/img/publication_preview/salkg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publication_preview/salkg.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="salkg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr> </div> </div> <div id="chan2021salkg" class="col-sm-8"> <div class="title">SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning</div> <div class="author"> <a href="https://aarzchan.com/" rel="external nofollow noopener" target="_blank">Aaron Chan</a>, <em>Jiashu Xu</em>, <a href="https://www.linkedin.com/in/boyuan-long-48a5ab16b/" rel="external nofollow noopener" target="_blank">Boyuan Long</a>, <a href="https://soumyasanyal.github.io/" rel="external nofollow noopener" target="_blank">Soumya Sanyal</a>, <a href="https://scholar.google.com/citations?user=wPYXV7gAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Tanishq Gupta</a>, and <a href="https://shanzhenren.github.io/" rel="external nofollow noopener" target="_blank">Xiang Ren</a> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2021/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/INK-USC/SalKG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Augmenting pre-trained language models with knowledge graphs (KGs) has achieved success on various commonsense reasoning tasks. However, for a given task instance, the KG, or certain parts of the KG, may not be useful. Although KG-augmented models often use attention to focus on specific KG components, the KG is still always used, and the attention mechanism is never explicitly taught which KG components should be used. Meanwhile, saliency methods can measure how much a KG feature (e.g., graph, node, path) influences the model to make the correct prediction, thus explaining which KG features are useful. This paper explores how saliency explanations can be used to improve KG-augmented models’ performance. First, we propose to create coarse (Is the KG useful?) and fine (Which nodes/paths in the KG are useful?) saliency explanations. Second, to motivate saliency-based supervision, we analyze oracle KG-augmented models which directly use saliency explanations as extra inputs for guiding their attention. Third, we propose SalKG, a framework for KG-augmented models to learn from coarse and/or fine saliency explanations. Given saliency explanations created from a task’s training set, SalKG jointly trains the model to predict the explanations, then solve the task by attending to KG features highlighted by the predicted explanations. On three commonsense QA benchmarks (CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can yield considerable performance gains – up to 2.76% absolute improvement on CSQA.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chan2021salkg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chan, Aaron and Xu, Jiashu and Long, Boyuan and Sanyal, Soumya and Gupta, Tanishq and Ren, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Jiashu Xu 徐家澍. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SM6XF0TG75"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SM6XF0TG75");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar. see latest work on google scholar page.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/files/Jiashu_Xu_CV.pdf"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-emojis-are-hard-to-interpret-and-judges-are-having-headaches",title:"Emojis are hard to interpret, and judges are having headaches \ud83e\udd15",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/Emoji/"}},{id:"post-test-for-randomness",title:"Test for randomness",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/Test_for_rand.pdf"}},{id:"post-bp-for-convolutional-layer",title:"BP for convolutional layer",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/BP_for_conv_layer.pdf"}},{id:"post-platonic-solids-and-graphs",title:"Platonic solids and graphs",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/Platonic_solids.pdf"}},{id:"post-one-probability-puzzle-of-pointer",title:"One probability puzzle of pointer",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/One_prob_puzzle_of_ptr.pdf"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%31%39%39%39%4A%30%36%31%35%75%6E%65@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-4093-2315","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=0uYehJsAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2110519123","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/cnut1648","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/jiashu-xu","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/JiashuXu2","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/267/0701.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>