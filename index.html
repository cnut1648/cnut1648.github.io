<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jiashu Xu ÂæêÂÆ∂Êæç </title> <meta name="author" content="Jiashu Xu ÂæêÂÆ∂Êæç"> <meta name="description" content="Personal website for Jiashu Xu "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, jiashu, jiashu-xu"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8C%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cnut1648.github.io//"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%31%39%39%39%4A%30%36%31%35%75%6E%65@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-4093-2315" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=0uYehJsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2110519123" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/cnut1648" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jiashu-xu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/JiashuXu2" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/267/0701.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/files/Jiashu_Xu_CV.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" i style="height: 3em"> <span class="font-weight-bold">Jiashu</span> Xu ÂæêÂÆ∂Êæç <img src="/assets/img/icon.jpg" style="height: 3em; float: right"> </h1> <p class="desc" style="margin-top: -3.5em; margin-bottom: 2em; font-style: italic;">Research Scientist at NVIDIA</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile-480.webp 480w,/assets/img/profile-800.webp 800w,/assets/img/profile-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/profile.jpg?75befdfb900bbf836be1939d45b5101b" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <style type="text/css">.ccimg{max-width:350px;width:100%;display:block;margin-left:auto;margin-right:auto}</style> <p>Hi there üëã</p> <p>I am a research scientist at <a href="https://research.nvidia.com/labs/dir/" rel="external nofollow noopener" target="_blank">NVIDIA Cosmos Team</a> <a href="https://github.com/NVIDIA/Cosmos" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/github/stars/NVIDIA/Cosmos?style=social" alt="GitHub stars"></a> working on generative model pre/post-training and physical ai. Before joining NVIDIA, I completed my Master‚Äôs at <a href="https://www.harvard.edu/" rel="external nofollow noopener" target="_blank">Harvard University</a> in Computer Science, following a double major in Applied Mathematics and Computer Science at <a href="https://www.usc.edu/" rel="external nofollow noopener" target="_blank">USC</a>. I have spent a bit of time as research intern at <a href="https://www.amazon.science/" rel="external nofollow noopener" target="_blank">Amazon Science</a> working on LLM and at <a href="https://www.microsoft.com/en-us/research/" rel="external nofollow noopener" target="_blank">Microsoft Research</a> working on synthetic data generation.</p> <p>My current research interests is in <code class="language-plaintext highlighter-rouge">Generative Models' Post-training</code>. Particularly,</p> <ul> <li>Post-train diffusion models [<a href="https://arxiv.org/pdf/2511.00062" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://arxiv.org/pdf/2501.03575" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://arxiv.org/pdf/2503.14492" rel="external nofollow noopener" target="_blank">3</a>, <a href="https://arxiv.org/pdf/2411.07135" rel="external nofollow noopener" target="_blank">4</a>] and VLMs [<a href="https://arxiv.org/pdf/2503.15558" rel="external nofollow noopener" target="_blank">5</a>, <a href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&amp;linkId=100000277255797" rel="external nofollow noopener" target="_blank">6</a>] w/ SFT and RL [<a href="https://arxiv.org/pdf/2512.04332" rel="external nofollow noopener" target="_blank">7</a>]</li> <li>Reliable model against malicious attacks [<a href="https://cnut1648.github.io/Model-Fingerprint/">8</a>, <a href="https://cnut1648.github.io/instruction-attack/">9</a>, <a href="https://arxiv.org/abs/2311.09763" rel="external nofollow noopener" target="_blank">10</a>]</li> <li>Excels in low-resource regimes via synthetic data generation [<a href="https://behavior-vision-suite.github.io/" rel="external nofollow noopener" target="_blank">11</a>, <a href="https://briannlongzhao.github.io/DreamDistribution/" rel="external nofollow noopener" target="_blank">12</a>, <a href="https://arxiv.org/abs/2309.05956" rel="external nofollow noopener" target="_blank">13</a>]</li> </ul> <p>This is my girlfriend üòç<a href="https://www.linkedin.com/in/carmen-liang/" rel="external nofollow noopener" target="_blank">Carmen</a> and me</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cc_and_me-480.webp 480w,/assets/img/cc_and_me-800.webp 800w,/assets/img/cc_and_me-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cc_and_me.jpg" class="img-fluid rounded z-depth-1 ccimg" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nvidia-cosmos-header-480.webp 480w,/assets/img/publication_preview/nvidia-cosmos-header-800.webp 800w,/assets/img/publication_preview/nvidia-cosmos-header-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/nvidia-cosmos-header.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="nvidia-cosmos-header.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="agarwal2025cosmos" class="col-sm-8"> <div class="title">Cosmos World Foundation Model Platform for Physical AI</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 <strong style="color: red;">(CES‚Äô25 Best of AI, Best Overall)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nvidia.com/en-us/ai/cosmos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2501.03575" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=9Uch931cDx8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/NVIDIA/Cosmos" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via NVIDIA Cosmos-Predict1.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">agarwal2025cosmos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cosmos World Foundation Model Platform for Physical AI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://www.nvidia.com/en-us/ai/cosmos/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cosmos-predict2.5-480.webp 480w,/assets/img/publication_preview/cosmos-predict2.5-800.webp 800w,/assets/img/publication_preview/cosmos-predict2.5-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/cosmos-predict2.5.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="cosmos-predict2.5.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="ali2025world" class="col-sm-8"> <div class="title">World simulation with video foundation models for physical ai</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://research.nvidia.com/labs/dir/cosmos-predict2.5/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2511.00062" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/nvidia-cosmos/cosmos-predict2.5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI visionlanguage model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5√ó smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ali2025world</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{World simulation with video foundation models for physical ai}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://research.nvidia.com/labs/dir/cosmos-predict2.5/}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cosmos-reason1-480.webp 480w,/assets/img/publication_preview/cosmos-reason1-800.webp 800w,/assets/img/publication_preview/cosmos-reason1-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/cosmos-reason1.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="cosmos-reason1.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://en.wikipedia.org/wiki/White_paper" rel="external nofollow noopener" target="_blank">White Paper</a></abbr> </div> </div> <div id="azzolini2025cosmos" class="col-sm-8"> <div class="title">Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nvidia.com/en-us/ai/cosmos/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2503.15558" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtube.com/live/TLzna9__DnI?feature=shared&amp;t=1993" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/nvidia-cosmos/cosmos-reason1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, CosmosReason1-7B and Cosmos-Reason1-56B. We curate data and train our models in two stages: Physical AI supervised fine-tuning (SFT) and Physical AI reinforcement learning (RL). To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and RL bring significant improvements. To facilitate the development of Physical AI, we make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">azzolini2025cosmos</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://www.nvidia.com/en-us/ai/cosmos/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ddrl-480.webp 480w,/assets/img/publication_preview/ddrl-800.webp 800w,/assets/img/publication_preview/ddrl-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/ddrl.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="ddrl.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr> </div> </div> <div id="ye2025data" class="col-sm-8"> <div class="title">Data-regularized Reinforcement Learning for Diffusion Models at Scale</div> <div class="author"> <a href="https://haotianye.com/" rel="external nofollow noopener" target="_blank">Haotian Ye</a>,¬†<a href="https://zhengkw18.github.io/" rel="external nofollow noopener" target="_blank">Kaiwen Zheng</a>,¬†<em>Jiashu Xu</em> ,¬†<a href="https://leo-li.com/" rel="external nofollow noopener" target="_blank">Puheng Li</a> ,¬†<a href="https://chendrag.github.io/" rel="external nofollow noopener" target="_blank">Huayu Chen</a>,¬†<a href="https://hanjq17.github.io/" rel="external nofollow noopener" target="_blank">Jiaqi Han</a> ,¬†<a href="https://shengliu66.github.io/" rel="external nofollow noopener" target="_blank">Sheng Liu</a> ,¬†<a href="https://qsh-zh.github.io/" rel="external nofollow noopener" target="_blank">Qinsheng Zhang</a>,¬†<a href="https://hanzimao.me/" rel="external nofollow noopener" target="_blank">Hanzi Mao</a>,¬†<a href="https://zekunhao.com/" rel="external nofollow noopener" target="_blank">Zekun Hao</a>,¬†<a href="https://prithv1.xyz/" rel="external nofollow noopener" target="_blank">Prithvijit Chattopadhyay</a> ,¬†<a href="https://dinghow.site/" rel="external nofollow noopener" target="_blank">Dinghao Yang</a>,¬†<a href="https://research.nvidia.com/labs/avg/author/liang-feng/" rel="external nofollow noopener" target="_blank">Liang Feng</a>,¬†<a href="https://github.com/foreverlms" rel="external nofollow noopener" target="_blank">Maosheng Liao</a>,¬†<a href="https://scholar.google.com/citations?user=gSB8_64AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Junjie Bai</a> ,¬†<a href="https://mingyuliu.net/" rel="external nofollow noopener" target="_blank">Ming-Yu Liu</a>,¬†<a href="https://www.james-zou.com/" rel="external nofollow noopener" target="_blank">James Zou</a>,¬†and¬†<a href="https://cs.stanford.edu/~ermon/" rel="external nofollow noopener" target="_blank">Stefano Ermon</a> </div> <div class="periodical"> <em>Arxiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://research.nvidia.com/labs/dir/ddrl/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/pdf/2512.04332" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ye2025data</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data-regularized Reinforcement Learning for Diffusion Models at Scale}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ye, Haotian and Zheng, Kaiwen and Xu, Jiashu and Li, Puheng and Chen, Huayu and Han, Jiaqi and Liu, Sheng and Zhang, Qinsheng and Mao, Hanzi and Hao, Zekun and Chattopadhyay, Prithvijit and Yang, Dinghao and Feng, Liang and Liao, Maosheng and Bai, Junjie and Liu, Ming-Yu and Zou, James and Ermon, Stefano}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Arxiv}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://research.nvidia.com/labs/dir/ddrl/}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fingerprintv2-480.webp 480w,/assets/img/publication_preview/fingerprintv2-800.webp 800w,/assets/img/publication_preview/fingerprintv2-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/fingerprintv2.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="fingerprintv2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu2024instructional" class="col-sm-8"> <div class="title">Instructional Fingerprinting of Large Language Models</div> <div class="author"> <em>Jiashu Xu</em>,¬†<a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang*</a> ,¬†<a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma*</a>,¬†<a href="https://koh.pw/" rel="external nofollow noopener" target="_blank">Pang Wei Koh</a>,¬†<a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>,¬†and¬†<a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , 2024 <strong style="color: red;">(Oral)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cnut1648.github.io/Model-Fingerprint/" class="btn btn-sm z-depth-0" role="button">Project Page</a> <a href="https://aclanthology.org/2024.naacl-long.180/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/cnut1648/Model-Fingerprint" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (\eg restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2024instructional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Instructional Fingerprinting of Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Wang, Fei and Ma, Mingyu Derek and Koh, Pang Wei and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Oral}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://cnut1648.github.io/Model-Fingerprint/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/BVS-480.webp 480w,/assets/img/publication_preview/BVS-800.webp 800w,/assets/img/publication_preview/BVS-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/BVS.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="BVS.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Highlight</abbr> </div> </div> <div id="ge2024behavior" class="col-sm-8"> <div class="title">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>,¬†<a href="https://tangyihe.com/" rel="external nofollow noopener" target="_blank">Yihe Tang*</a>,¬†<em>Jiashu Xu*</em>,¬†<a href="https://www.cemgokmen.com/" rel="external nofollow noopener" target="_blank">Cem Gokmen*</a> ,¬†<a href="https://www.chengshuli.me/" rel="external nofollow noopener" target="_blank">Chengshu Li</a>,¬†<a href="https://wensi-ai.github.io/" rel="external nofollow noopener" target="_blank">Wensi Ai</a>,¬†<a href="https://web.stanford.edu/~benjm/" rel="external nofollow noopener" target="_blank">Benjamin Jose Martinez</a>,¬†<a href="https://www.linkedin.com/in/arman-aydin-915035185/" rel="external nofollow noopener" target="_blank">Arman Aydin</a>,¬†<a href="https://www.linkedin.com/in/mona-anvari/" rel="external nofollow noopener" target="_blank">Mona Anvari</a>,¬†<a href="https://scholar.google.ca/citations?user=u4S8E4UAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ayush K Chakravarthy</a>,¬†<a href="https://kovenyu.com/" rel="external nofollow noopener" target="_blank">Hong-Xing Yu</a>,¬†<a href="https://jdw.ong/" rel="external nofollow noopener" target="_blank">Josiah Wong</a>,¬†<a href="https://scholar.google.com/citations?user=sqTh_dwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sanjana Srivastava</a>,¬†<a href="https://scholar.google.com/citations?hl=en&amp;user=jGwt3mcAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sharon Lee</a>,¬†<a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Shengxin Zha</a>,¬†<a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a> ,¬†<a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>,¬†<a href="https://robertomartinmartin.com/" rel="external nofollow noopener" target="_blank">Roberto Martin-Martin</a> ,¬†<a href="https://aptx4869lm.github.io/" rel="external nofollow noopener" target="_blank">Miao Liu</a>,¬†<a href="https://pzzhang.github.io/pzzhang/" rel="external nofollow noopener" target="_blank">Pengchuan Zhang</a> ,¬†<a href="https://ai.stanford.edu/~zharu/" rel="external nofollow noopener" target="_blank">Ruohan Zhang</a>,¬†<a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>,¬†and¬†<a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2024 <strong style="color: red;">(Highlight)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://behavior-vision-suite.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as "filled" and "folded"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ge2024behavior</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Tang, Yihe and Xu, Jiashu and Gokmen, Cem and Li, Chengshu and Ai, Wensi and Martinez, Benjamin Jose and Aydin, Arman and Anvari, Mona and Chakravarthy, Ayush K and Yu, Hong-Xing and Wong, Josiah and Srivastava, Sanjana and Lee, Sharon and Zha, Shengxin and Itti, Laurent and Li, Yunzhu and Martin-Martin, Roberto and Liu, Miao and Zhang, Pengchuan and Zhang, Ruohan and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Highlight}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://behavior-vision-suite.github.io/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/genusd-480.webp 480w,/assets/img/publication_preview/genusd-800.webp 800w,/assets/img/publication_preview/genusd-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/genusd.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="genusd.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://www.siggraph.org/" rel="external nofollow noopener" target="_blank">SIGGRAPH</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Real-Time Live!</abbr> </div> </div> <div id="lin2024genusd" class="col-sm-8"> <div class="title">Genusd: 3d scene generation made easy</div> <div class="author"> NVIDIA Cosmos Team: <em>Jiashu Xu</em> </div> <div class="periodical"> <em>In ACM SIGGRAPH</em> , 2024 <strong style="color: red;">(Real-Time Live!)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&amp;linkId=100000277255797" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://www.youtube.com/watch?v=Gm1B5DT8kE0&amp;t=1972s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We introduce GenUSD, an end-to-end text-to-scene generation framework that transforms natural language queries into realistic 3D scenes, including 3D objects and layouts. The process involves two main steps: 1) A Large Language Model (LLM) generates a scene layout hierarchically. It first proposes a high-level plan to decompose the scene into multiple functionally and spatially distinct subscenes. Then, for each subscene, the LLM proposes objects with detailed positions, poses, sizes, and descriptions. To manage complex object relationships and intricate scenes, we introduce object layout design meta functions as tools for the LLM. 2) A novel text-to-3D model generates each 3D object with surface meshes and high-resolution texture maps based on the LLM‚Äôs descriptions. The assembled 3D assets form the final 3D scene, represented as a Universal Scene Description (USD) format. GenUSD ensures physical plausibility by incorporating functions to prevent collisions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@incollection</span><span class="p">{</span><span class="nl">lin2024genusd</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Genusd: 3d scene generation made easy}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Team, NVIDIA Cosmos}</span><span class="p">,</span>
  <span class="na">author_display</span> <span class="p">=</span> <span class="s">{NVIDIA Cosmos Team: &lt;em&gt;Jiashu Xu&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Real-Time Live!}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--2}</span><span class="p">,</span>
  <span class="na">project_page</span> <span class="p">=</span> <span class="s">{https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&amp;linkId=100000277255797}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/can-nli-480.webp 480w,/assets/img/publication_preview/can-nli-800.webp 800w,/assets/img/publication_preview/can-nli-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/can-nli.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="can-nli.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu-etal-2023-nli" class="col-sm-8"> <div class="title">Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?</div> <div class="author"> <em>Jiashu Xu</em> ,¬†<a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>,¬†and¬†<a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Association for Computational Linguistics (ACL)</em> , Jul 2023 <strong style="color: red;">(Oral)</strong> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2023.acl-long.138.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/luka-group/nli_as_indirect_supervision" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2023-nli</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can {NLI} Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Ma, Mingyu Derek and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Oral}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.138}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.138}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2450--2467}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%31%39%39%39%4A%30%36%31%35%75%6E%65@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-4093-2315" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=0uYehJsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2110519123" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/cnut1648" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jiashu-xu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/JiashuXu2" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/267/0701.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </div> <div class="contact-note">Feel free to contact me in email! </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Jiashu Xu ÂæêÂÆ∂Êæç. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SM6XF0TG75"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SM6XF0TG75");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar. see latest work on google scholar page.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/files/Jiashu_Xu_CV.pdf"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-emojis-are-hard-to-interpret-and-judges-are-having-headaches",title:"Emojis are hard to interpret, and judges are having headaches \ud83e\udd15",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/Emoji/"}},{id:"post-test-for-randomness",title:"Test for randomness",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/Test_for_rand.pdf"}},{id:"post-bp-for-convolutional-layer",title:"BP for convolutional layer",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/BP_for_conv_layer.pdf"}},{id:"post-platonic-solids-and-graphs",title:"Platonic solids and graphs",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/Platonic_solids.pdf"}},{id:"post-one-probability-puzzle-of-pointer",title:"One probability puzzle of pointer",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/One_prob_puzzle_of_ptr.pdf"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%31%39%39%39%4A%30%36%31%35%75%6E%65@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-4093-2315","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=0uYehJsAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2110519123","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/cnut1648","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/jiashu-xu","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/JiashuXu2","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/267/0701.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>